{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import json, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_liar_raw(path: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function to preprocess the LIAR-RAW dataset.\n",
    "\n",
    "    It includes:\n",
    "    - Remove the \"reports\" column.\n",
    "    - Rename the labels \"mostly-true\" to \"true\" and \"barely-true\" and \"pants-fire\" to \"false\".\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the LIAR-RAW dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Train, validation and test datasets.\n",
    "    \"\"\"\n",
    "    train = pd.read_json(path+\"train.json\", orient=\"records\")\n",
    "    valid = pd.read_json(path+\"val.json\", orient=\"records\")\n",
    "    test = pd.read_json(path+\"test.json\", orient=\"records\")\n",
    "\n",
    "    # Remove the \"reports\" column\n",
    "    train.drop(columns=[\"reports\"], inplace=True)\n",
    "    valid.drop(columns=[\"reports\"], inplace=True)\n",
    "    test.drop(columns=[\"reports\"], inplace=True)\n",
    "\n",
    "    # Rename the labels\n",
    "    train[\"label\"] = train[\"label\"].replace({\"mostly-true\": \"true\", \"barely-true\": \"false\", \"pants-fire\": \"false\"})\n",
    "    valid[\"label\"] = valid[\"label\"].replace({\"mostly-true\": \"true\", \"barely-true\": \"false\", \"pants-fire\": \"false\"})\n",
    "    test[\"label\"] = test[\"label\"].replace({\"mostly-true\": \"true\", \"barely-true\": \"false\", \"pants-fire\": \"false\"})\n",
    "\n",
    "    return train, valid, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rawfc(path: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function to preprocess the RAWFC dataset.\n",
    "\n",
    "    It includes:\n",
    "    - Go to the split folder.\n",
    "    - Read the JSON files.\n",
    "    - Concatenate the files into a single DataFrame.\n",
    "    - Repeat with the other splits.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the RAWFC dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple [pd.DataFrame, pd.DataFrame, pd.DataFrame]: Train, validation and test datasets.\n",
    "    \"\"\"\n",
    "    train = pd.DataFrame(columns=[\"event_id\", \"claim\", \"label\", \"explain\"])\n",
    "    valid = pd.DataFrame(columns=[\"event_id\", \"claim\", \"label\", \"explain\"])\n",
    "    test = pd.DataFrame(columns=[\"event_id\", \"claim\", \"label\", \"explain\"])\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        current_path = path + split + \"/\"\n",
    "        json_files = [file for file in os.listdir(current_path) if file.endswith(\".json\")]\n",
    "\n",
    "        for index, file in enumerate(json_files):\n",
    "            json_data = json.load(open(current_path + file))\n",
    "\n",
    "            # get event_id, claim, label, explain, and original_label\n",
    "            event_id = json_data[\"event_id\"]+\".json\"\n",
    "            claim = json_data[\"claim\"]\n",
    "            label = json_data[\"label\"]\n",
    "            original_label = json_data[\"original_label\"]\n",
    "            explain = json_data[\"explain\"]\n",
    "\n",
    "            if split == \"train\":\n",
    "                train.loc[index]= [event_id, claim, label, explain]\n",
    "            elif split == \"val\":\n",
    "                valid.loc[index]= [event_id, claim, label, explain]\n",
    "            else:\n",
    "                test.loc[index]= [event_id, claim, label, explain]\n",
    "\n",
    "    train[\"label\"] = train[\"label\"].replace({\"half\": \"half-true\"})\n",
    "    valid[\"label\"] = valid[\"label\"].replace({\"half\": \"half-true\"})\n",
    "    test[\"label\"] = test[\"label\"].replace({\"half\": \"half-true\"})\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.7 s, sys: 1.32 s, total: 7.02 s\n",
      "Wall time: 7.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# liar-raw dataset\n",
    "path = \"../dataset/LIAR-RAW/\"\n",
    "train_liar_raw, valid_liar_raw, test_liar_raw = preprocess_liar_raw(path)\n",
    "\n",
    "# rawfc dataset\n",
    "path = \"../dataset/RAWFC/\"\n",
    "train_rawfc, valid_rawfc, test_rawfc = preprocess_rawfc(path)\n",
    "\n",
    "# merge the datasets\n",
    "train_df = pd.concat([train_liar_raw, train_rawfc])\n",
    "valid_df = pd.concat([valid_liar_raw, valid_rawfc])\n",
    "test_df = pd.concat([test_liar_raw, test_rawfc])\n",
    "\n",
    "# convert df to dataset\n",
    "train_ds = Dataset.from_pandas(train_df, split=\"train\")\n",
    "valid_ds = Dataset.from_pandas(valid_df, split=\"validation\")\n",
    "test_ds = Dataset.from_pandas(test_df, split=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f4beca142c4026ad49f80697442c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f03e20045e45efb7d72dbed0139ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2626eccb35413aa1d1322b033bc600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# merge into a single dataset and save it\n",
    "final_dataset = DatasetDict({\"train\": train_ds, \"validation\": valid_ds, \"test\": test_ds})\n",
    "final_dataset.save_to_disk(\"../dataset/LIAR_RAW_RAWFC_Merged/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa21e3c6f8ee4c2ba78243be1a40a02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc6fde7a89a41c89cba4579835ddbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8322e3b1e98a4d5baeacafe76f57a0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b025697d562d4dafa39ba54d6e68737f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00e51342e6044e28a7b6652488a4c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f6369a650d4773b19862904e1237b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/NoAtmosphere0/LIAR_RAW_RAWFC_Merged/commit/391f2f64feb989f823484ea4325f48a6c1c6b2fe', commit_message='Upload dataset', commit_description='', oid='391f2f64feb989f823484ea4325f48a6c1c6b2fe', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/NoAtmosphere0/LIAR_RAW_RAWFC_Merged', endpoint='https://huggingface.co', repo_type='dataset', repo_id='NoAtmosphere0/LIAR_RAW_RAWFC_Merged'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to hub\n",
    "final_dataset.push_to_hub(repo_id=\"NoAtmosphere0/LIAR_RAW_RAWFC_Merged\",\n",
    "                          \n",
    "                          private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push individual splits\n",
    "train_ds.push_to_hub(repo_id=\"NoAtmosphere0/LIAR_RAW_RAWFC_Merged\", name=\"train\", private=True)\n",
    "valid_ds.push_to_hub(repo_id=\"NoAtmosphere0/LIAR_RAW_RAWFC_Merged\", name=\"validation\", private=True)\n",
    "test_ds.push_to_hub(repo_id=\"NoAtmosphere0/LIAR_RAW_RAWFC_Merged\", name=\"test\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
