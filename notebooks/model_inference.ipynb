{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference and Visualization Notebook\n",
    "\n",
    "This notebook loads saved models and runs inference on evaluation data, then visualizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from TextUMC.model_dbscan import TextUMC, ModelConfig, Evidence, Claim, cluster_and_evaluate, visualize_clusters\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load a saved model from the outputs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available output directories:\n",
      "- 20250114_232554\n",
      "- 20250115_001858\n",
      "- 20250115_160930\n",
      "- 20250115_161613\n",
      "- 20250115_162201\n",
      "- 20250115_162315\n",
      "- 20250115_162539\n",
      "- 20250115_163156\n",
      "- 20250115_163326\n",
      "- 20250115_165133\n",
      "- 20250115_170200\n",
      "- 20250115_171836\n",
      "- 20250115_172108\n",
      "- 20250115_172948\n",
      "- 20250115_173045\n",
      "- 20250115_175745\n",
      "- 20250115_175805\n",
      "- 20250115_180112\n",
      "- 20250115_180146\n",
      "- 20250115_180230\n",
      "- 20250115_204304\n",
      "- 20250115_204409\n",
      "- 20250115_204728\n",
      "- 20250115_204921\n",
      "- 20250115_205108\n",
      "- 20250115_205818\n",
      "- 20250115_205908\n",
      "- 20250115_210556\n",
      "- 20250115_210850\n",
      "- 20250115_215300\n",
      "- 20250116_004304\n",
      "- 20250116_004831\n",
      "- 20250116_004930\n",
      "- 20250116_005208\n",
      "- 20250116_051048\n",
      "- 20250116_102540\n",
      "- 20250116_102608\n",
      "- 20250116_154842\n",
      "- 20250116_155523\n",
      "- 20250116_155600\n",
      "- 20250116_155614\n",
      "- 20250116_155628\n",
      "- 20250116_155715\n",
      "- 20250116_155725\n",
      "- 20250120_160124\n",
      "- 20250120_160132\n",
      "- 20250120_161320\n",
      "- 20250120_161432\n",
      "- 20250120_161550\n",
      "- 20250120_161659\n",
      "- 20250120_161901\n",
      "- 20250120_162006\n",
      "- 20250120_162740\n",
      "- 20250120_163227\n",
      "- 20250120_164050\n",
      "- 20250120_164307\n",
      "- 20250120_191507\n",
      "- 20250120_191528\n",
      "- 20250121_102540\n",
      "- 20250121_103636\n",
      "- 20250121_105820\n",
      "- 20250121_111738\n",
      "- 20250121_111959\n",
      "- 20250121_112052\n",
      "- 20250121_112114\n",
      "- 20250121_113736\n",
      "- 20250121_235947\n",
      "- 20250122_000339\n",
      "- 20250122_000414\n",
      "- 20250122_002507\n",
      "- 20250122_003922\n",
      "- 20250122_004309\n",
      "- 20250122_004650\n",
      "- 20250122_004807\n",
      "- 20250122_004935\n",
      "- 20250122_005547\n",
      "- 20250122_010056\n",
      "- 20250122_233233\n",
      "- 20250123_000227\n",
      "- 20250123_000729\n",
      "- 20250123_001042\n",
      "- 20250123_003643\n",
      "- 20250123_010821\n",
      "- 20250123_010951\n",
      "- 20250123_011352\n",
      "- 20250123_011743\n",
      "- 20250123_012332\n",
      "- 20250123_012608\n",
      "- 20250123_012643\n",
      "- 20250123_012824\n",
      "- 20250123_012956\n",
      "- 20250123_013618\n",
      "- 20250123_014116\n",
      "- 20250123_114850\n",
      "- 20250123_114938\n",
      "- 20250123_115239\n",
      "- 20250123_123727\n",
      "- 20250123_123834\n",
      "- 20250123_233700\n",
      "- 20250123_233719\n",
      "- 20250124_001129\n",
      "- 20250124_001155\n",
      "- 20250124_001502\n",
      "- 20250124_001645\n",
      "- 20250124_001829\n",
      "- 20250124_001909\n",
      "- 20250124_010653\n",
      "- 20250124_010816\n",
      "- 20250124_010852\n",
      "- 20250124_011334\n",
      "- 20250124_011418\n",
      "- 20250124_014514\n",
      "- 20250124_014813\n",
      "- 20250124_015112\n",
      "- 20250124_015140\n",
      "- 20250124_103300\n",
      "- 20250124_103317\n",
      "- 20250124_103450\n",
      "- 20250124_110157\n",
      "- 20250124_220605\n",
      "- 20250124_220727\n",
      "- 20250124_221145\n",
      "- 20250124_221506\n",
      "- 20250125_004111\n",
      "- 20250125_095253\n",
      "- 20250125_101649\n",
      "- 20250125_102431\n",
      "- 20250125_102631\n",
      "- 20250125_102810\n",
      "- 20250125_102956\n",
      "- 20250125_103016\n",
      "- 20250125_104449\n",
      "- 20250125_104534\n",
      "- 20250125_105921\n",
      "- 20250125_214949\n",
      "- 20250125_220145\n",
      "- 20250125_220245\n",
      "- 20250125_221714\n",
      "- 20250125_222227\n",
      "- 20250125_222535\n",
      "- 20250125_224119\n",
      "- 20250125_224246\n",
      "- 20250125_224740\n",
      "- 20250125_225132\n",
      "- 20250125_225708\n",
      "- 20250125_225902\n",
      "- 20250125_230127\n",
      "- 20250125_230308\n",
      "- 20250125_230459\n",
      "- 20250125_230752\n",
      "- 20250125_231349\n",
      "- 20250125_231605\n",
      "- 20250125_231735\n",
      "- 20250125_232052\n",
      "- 20250125_232520\n",
      "- 20250125_232629\n",
      "- 20250125_233214\n",
      "- 20250125_233328\n",
      "- 20250125_234918\n",
      "- 20250125_235505\n",
      "- 20250126_000615\n",
      "- 20250126_000656\n",
      "- 20250126_001431\n",
      "- 20250126_001704\n",
      "- 20250126_002043\n",
      "- 20250126_002429\n",
      "- 20250126_003920\n",
      "- 20250126_004008\n",
      "- 20250126_004101\n",
      "- 20250126_004430\n",
      "- 20250126_180858\n",
      "- 20250126_202618\n",
      "- 20250126_202855\n",
      "- 20250126_203532\n",
      "- 20250127_092957\n",
      "- 20250127_094211\n",
      "- 20250127_095721\n",
      "- 20250127_102536\n",
      "- 20250127_102638\n",
      "- 20250127_103117\n",
      "- 20250127_105542\n",
      "- 20250127_111040\n",
      "- 20250127_111335\n",
      "- 20250127_111604\n",
      "- 20250130_092256\n",
      "- 20250130_092638\n",
      "- 20250130_092717\n",
      "- 20250130_093032\n",
      "- 20250130_093218\n",
      "- 20250130_093421\n",
      "- 20250130_095027\n",
      "- 20250130_104033\n",
      "- 20250130_104312\n",
      "- 20250130_114016\n",
      "- 20250130_114041\n",
      "- 20250130_114821\n",
      "- 20250130_115316\n",
      "- 20250130_115441\n",
      "- 20250130_115839\n",
      "- 20250130_161127\n",
      "- 20250130_161151\n",
      "- 20250130_161232\n",
      "- 20250130_171648\n",
      "- 20250130_171706\n",
      "- 20250130_204453\n",
      "- 20250130_210712\n",
      "- 20250130_210944\n",
      "- 20250130_212253\n",
      "- 20250130_214821\n",
      "- 20250131_081436\n",
      "- 20250131_094935\n",
      "- 20250131_095402\n",
      "- 20250131_100426\n",
      "- 20250131_100907\n",
      "- 20250131_111002\n",
      "- 20250131_112102\n",
      "- 20250131_112457\n",
      "- 20250131_112547\n",
      "- 20250131_112647\n",
      "- 20250131_114023\n",
      "- 20250131_114452\n",
      "- 20250131_115625\n",
      "- 20250131_121758\n",
      "- 20250131_123159\n",
      "\n",
      "Loaded model from ../outputs/20250131_123159/textumc_dbscan_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with default config\n",
    "model = TextUMC()\n",
    "\n",
    "# List available model checkpoints\n",
    "output_dirs = sorted([d for d in os.listdir('../outputs') if os.path.isdir(os.path.join('../outputs', d))])\n",
    "print(\"Available output directories:\")\n",
    "for d in output_dirs:\n",
    "    print(f\"- {d}\")\n",
    "\n",
    "# Load the latest model by default\n",
    "latest_dir = output_dirs[-1] if output_dirs else None\n",
    "if latest_dir:\n",
    "    model_path = os.path.join('../outputs', latest_dir, 'textumc_dbscan_model.pt')\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_model(model_path)\n",
    "        print(f\"\\nLoaded model from {model_path}\")\n",
    "    else:\n",
    "        print(f\"\\nNo model_final.pt found in {latest_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Data\n",
    "\n",
    "Load the evaluation data to run inference on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1451 claims for evaluation\n"
     ]
    }
   ],
   "source": [
    "def load_evaluation_data(eval_path='../dataset/test.json'):\n",
    "    \"\"\"Load and prepare evaluation data\"\"\"\n",
    "    with open(eval_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    claims = []\n",
    "    for item in data:\n",
    "        # Create Evidence objects\n",
    "        evidences = [Evidence(evidence['report_id'], evidence['content']) \n",
    "                    for evidence in item['reports']]\n",
    "        \n",
    "        # Create Claim object\n",
    "        claim = Claim(\n",
    "            claim_id=str(item.get('event_id', '')),\n",
    "            content=item['claim'],\n",
    "            label=item.get('label', ''),\n",
    "            explanation=item.get('explain', ''),\n",
    "            evidences=evidences\n",
    "        )\n",
    "        claims.append(claim)\n",
    "    \n",
    "    return claims\n",
    "\n",
    "# Load evaluation data\n",
    "eval_claims = load_evaluation_data()\n",
    "print(f\"Loaded {len(eval_claims)} claims for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([26, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1395, -0.0268,  0.0509,  ..., -0.1060, -0.0308, -0.0174],\n",
       "        [-0.0055,  0.0271, -0.1331,  ..., -0.0378, -0.1289,  0.0627],\n",
       "        [ 0.1287,  0.0716, -0.0469,  ...,  0.0583,  0.0131,  0.0339],\n",
       "        ...,\n",
       "        [ 0.0084,  0.0929,  0.1319,  ...,  0.0017,  0.0391,  0.0515],\n",
       "        [-0.1156,  0.1822, -0.0038,  ..., -0.0488,  0.0004, -0.0187],\n",
       "        [ 0.0466,  0.1041, -0.0269,  ...,  0.0179, -0.1794,  0.1510]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_texts = [ev.content for ev in eval_claims[0].evidences]\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(evidence_texts)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Run the model on evaluation data and get clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to ../eval_outputs/20250131_174842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>eps</th>\n",
       "      <th>min_samples</th>\n",
       "      <th>pca_components</th>\n",
       "      <th>explained_variance_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11972.json</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878845</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.518979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim_id  n_clusters       eps  min_samples  pca_components  \\\n",
       "0  11972.json           1  0.878845            3               7   \n",
       "\n",
       "   explained_variance_ratio  \n",
       "0                  0.518979  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_inference(model, claims, save_dir='../eval_outputs'):\n",
    "    \"\"\"Run inference on claims and save results\"\"\"\n",
    "    # Create output directory with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = os.path.join(save_dir, timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    if claims is not list:\n",
    "        claims = [claims]\n",
    "    \n",
    "    results = []\n",
    "    for claim in claims:\n",
    "        # Get evidence texts\n",
    "        evidence_texts = [ev.content for ev in claim.evidences]\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(evidence_texts)\n",
    "        \n",
    "        # Perform clustering\n",
    "        labels, metrics = cluster_and_evaluate(embeddings)\n",
    "        \n",
    "        # Visualize clusters\n",
    "        visualize_clusters(\n",
    "            embeddings=embeddings,\n",
    "            labels=labels,\n",
    "            title=f'Claim_{claim.claim_id}',\n",
    "            save_path=run_dir\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'claim_id': claim.claim_id,\n",
    "            'n_clusters': metrics['n_clusters'],\n",
    "            'eps': metrics['eps'],\n",
    "            'min_samples': metrics['min_samples'],\n",
    "            'pca_components': metrics['pca_components'],\n",
    "            'explained_variance_ratio': metrics['explained_variance_ratio']\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(run_dir, 'inference_results.csv'), index=False)\n",
    "    \n",
    "    return results_df, run_dir\n",
    "\n",
    "# Run inference\n",
    "results_df, run_dir = run_inference(model, eval_claims[0])\n",
    "print(f\"\\nResults saved to {run_dir}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Analyze and visualize the inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_distribution(results_df):\n",
    "    \"\"\"Plot distribution of number of clusters\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    results_df['n_clusters'].hist(bins=20)\n",
    "    plt.title('Distribution of Number of Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_correlation(results_df):\n",
    "    \"\"\"Plot correlation between different metrics\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(results_df['eps'], results_df['n_clusters'])\n",
    "    plt.title('Epsilon vs Number of Clusters')\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.show()\n",
    "\n",
    "# Plot analysis\n",
    "plot_cluster_distribution(results_df)\n",
    "plot_metrics_correlation(results_df)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(results_df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
