"""
The hallucination_checker module is used to check if a model has hallucinated or not.

-----
Input: Synthesized text generated by the synthesizer module 

Output: Boolean value indicating whether the model has hallucinated or not. 

- If True, the synthesizer will generate a new response.

- If False, the synthesizer will return the response as it is.
----- 

In this module, we use the vectara/hallucination_evaluation_model

The HallucinationChecker class is used to check if a model has hallucinated or not.

A hallucination occurs when the response is coherent but factually incorrect or nonsensical outputs that are not grounded in the provided context.
"""

from transformers import AutoModelForCausalLM, pipeline, AutoTokenizer
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class HallucinationChecker:
    """
    The HallucinationChecker class is used to check if a model has hallucinated or not.
    A hallucination occurs when the response is coherent but factually incorrect or nonsensical
    outputs that are not grounded in the provided context.
    """

    def __init__(self, model_path=None, tokenizer_path=None):
        """Initializes the HallucinationChecker class with the model and tokenizer

        The base model is the Phi-3.5-mini-instruct model from Microsoft
        The adapter model is the phi3.5-hallucination-judge model from grounded-ai
        """
        # self.model = AutoModelForCausalLM.from_pretrained(
        #     "microsoft/Phi-3.5-mini-instruct", trust_remote_code=True
        # )
        # self.config = PeftConfig.from_pretrained(
        #     "grounded-ai/phi3.5-hallucination-judge"
        # )
        # self.base_model = AutoModelForCausalLM.from_pretrained(
        #     "microsoft/Phi-3.5-mini-instruct"
        # )
        # self.model = PeftModel.from_pretrained(
        #     self.base_model, "grounded-ai/phi3.5-hallucination-judge"
        # )
        # self.tokenizer = AutoTokenizer.from_pretrained(
        #     "microsoft/Phi-3.5-mini-instruct"
        # )

        if model_path and tokenizer_path:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path, trust_remote_code=True
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                "../models/SmolLM2-1.7B-Instruct"
            ).to(device)
            self.tokenizer = AutoTokenizer.from_pretrained(
                "../models/SmolLM2-1.7B-Instruct"
            )

    def check_hallucination(self, reference: str, query: str, response: str) -> bool:
        """
        Checks if a model has hallucinated or not.

        Args:
            reference (str): The reports or information that the model uses to evaluate the response
            query (str): A cluster of semantically similar reports or information on a claim
            response (str): Synthesized text generated by the synthesizer module

        Returns:
            bool: True if the model has hallucinated, False otherwise
        """

        def format_input(reference: str, query: str, response: str):
            """
            Formats the input for the hallucination checker model.

            Args:
                reference (str): The reports or information that the model uses to evaluate the response
                query (str): A cluster of semantically similar reports or information on a claim
                response (str): Synthesized text generated by the synthesizer module

            Returns:
                str: The formatted prompt for the hallucination checker model
            """
            prompt = f"""Your job is to evaluate whether a machine learning model has hallucinated or not.
            A hallucination occurs when the response is coherent but factually incorrect or nonsensical
            outputs that are not grounded in the provided context.
            You are given the following information:
            ####INFO####
            [Knowledge]: {reference}
            [User Input]: {query}
            [Model Response]: {response}
            ####END INFO####
            Based on the information provided is the model output a hallucination? Respond with only "yes" or "no"
            """
            return prompt

        # text = format_input(reference, query, response)

        # messages = [{"role": "user", "content": text}]

        # pipe = pipeline(
        #     "text-generation",
        #     model=self.base_model,
        #     model_kwargs={
        #         "attn_implementation": self.attn_implementation,
        #         "torch_dtype": torch.float16,
        #     },
        #     tokenizer=self.tokenizer,
        # )
        # generation_args = {
        #     "max_new_tokens": 2,
        #     "return_full_text": False,
        #     "temperature": 0.01,
        #     "do_sample": True,
        # }

        # output = pipe(messages, **generation_args)

        # # return True if the model output is "yes" and False if the model output is "no"
        # return output["generated_text"].strip().lower() == "yes"

        #! Need to change the methods, right now always returning False
        return False
